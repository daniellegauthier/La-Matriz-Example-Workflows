{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfaf66a0-a6d9-42e8-a6c7-659da32c7b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: sentencepiece in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: filelock in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.13/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch sentencepiece beautifulsoup4 requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8d39f80f-a0f9-42cd-84ad-bea9b2554422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: sentencepiece in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (4.12.3)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.13/site-packages (2.32.5)\n",
      "Requirement already satisfied: filelock in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.13/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.13/site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"identity\": {\n",
      "    \"name\": \"governments\",\n",
      "    \"organization\": \"community group\"\n",
      "  },\n",
      "  \"intersection\": {\n",
      "    \"expression\": \"Larry Summers\\u2019s involvement with Jeffrey Epstein goes back to at least 1998. He maintained his contact with Epstein until July 5, 2019, the day before his arrest. The question is: Did everyone miss the connection and its seriousness, or did they choose to ignore it?\",\n",
      "    \"body_experience\": \"health impact\",\n",
      "    \"intention\": \"adaptation\"\n",
      "  },\n",
      "  \"demographic\": {\n",
      "    \"race\": \"minority\",\n",
      "    \"gender\": \"unspecified\",\n",
      "    \"education\": \"traditional knowledge\",\n",
      "    \"occupation\": \"policy makers\",\n",
      "    \"ability\": \"able\",\n",
      "    \"wealth\": \"mixed\",\n",
      "    \"health\": \"at risk\"\n",
      "  },\n",
      "  \"intention\": {\n",
      "    \"autonomy\": \"externally imposed\",\n",
      "    \"design\": \"co-created\",\n",
      "    \"future\": \"Larry Summers\\u2019s involvement with Jeffrey Epstein goes back to at least 1998. He maintained his contact with Epstein until July 5, 2019, the day before his arrest. The question is: Did everyone miss the connection and its seriousness, or did they choose to ignore it?\",\n",
      "    \"new\": \"social innovation\",\n",
      "    \"symbol\": \"progress\",\n",
      "    \"impact\": \"localized change\",\n",
      "    \"support\": \"institutional support\"\n",
      "  },\n",
      "  \"policy_critique\": {\n",
      "    \"origin\": \"community-led\",\n",
      "    \"leak\": \"resource extraction\",\n",
      "    \"depletion\": \"social depletion\",\n",
      "    \"obstacles\": \"cultural barriers\"\n",
      "  },\n",
      "  \"future\": {\n",
      "    \"ritual\": \"remembrance\",\n",
      "    \"hope\": \"Larry Summers\\u2019s involvement with Jeffrey Epstein goes back to at least 1998. He maintained his contact with Epstein until July 5, 2019. That is a cumulative 22 years, a considerable amount of time by any standard. The question is: Did everyone miss the connection and its seriousness,\",\n",
      "    \"depict\": \"transitional\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. INSTALL DEPENDENCIES (run once)\n",
    "# =========================================================\n",
    "!pip install transformers torch sentencepiece beautifulsoup4 requests\n",
    "\n",
    "# =========================================================\n",
    "# 2. IMPORTS\n",
    "# =========================================================\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import copy\n",
    "\n",
    "# =========================================================\n",
    "# 3. LOAD PLURIVERSE JSON SCHEMA\n",
    "# =========================================================\n",
    "SCHEMA_PATH = \"Pluriverse_news_data.json\"\n",
    "\n",
    "with open(SCHEMA_PATH, \"r\") as f:\n",
    "    PLURIVERSE_SCHEMA = json.load(f)\n",
    "\n",
    "# =========================================================\n",
    "# 4. LOAD TRANSFORMER MODELS\n",
    "# =========================================================\n",
    "classifier = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\"\n",
    ")\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\"\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 5. SCRAPE ARTICLE FROM URL\n",
    "# =========================================================\n",
    "def scrape_article(url):\n",
    "    response = requests.get(url, timeout=10)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    paragraphs = [p.get_text() for p in soup.find_all(\"p\")]\n",
    "    return \" \".join(paragraphs)\n",
    "\n",
    "# =========================================================\n",
    "# 6. NLP HELPER FUNCTIONS\n",
    "# =========================================================\n",
    "def clean_article_text(text):\n",
    "    BLOCKED_PHRASES = [\n",
    "        \"enable javascript\",\n",
    "        \"disable any ad blocker\",\n",
    "        \"we noticed you are using\",\n",
    "        \"subscribe to continue\",\n",
    "        \"sign up to read\",\n",
    "        \"cookies are required\"\n",
    "    ]\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in text.split(\"\\n\"):\n",
    "        line = line.strip()\n",
    "        if len(line) < 40:\n",
    "            continue\n",
    "        if any(bp in line.lower() for bp in BLOCKED_PHRASES):\n",
    "            continue\n",
    "        cleaned_lines.append(line)\n",
    "\n",
    "    return \" \".join(cleaned_lines)\n",
    "\n",
    "\n",
    "def summarize_text(text, max_len=60):\n",
    "    if not text or len(text.split()) < 40:\n",
    "        return \"insufficient textual signal\"\n",
    "\n",
    "    try:\n",
    "        summary = summarizer(\n",
    "            text[:3000],\n",
    "            max_length=max_len,\n",
    "            min_length=20,\n",
    "            do_sample=False\n",
    "        )\n",
    "        return summary[0][\"summary_text\"]\n",
    "    except Exception:\n",
    "        return \"summary unavailable\"\n",
    "\n",
    "\n",
    "def prompted_summary(prompt, text, max_len=60):\n",
    "    combined = f\"{prompt}\\n\\n{text}\"\n",
    "    summary = summarizer(\n",
    "        combined[:3000],\n",
    "        max_length=max_len,\n",
    "        min_length=20,\n",
    "        do_sample=False\n",
    "    )\n",
    "    return summary[0][\"summary_text\"]\n",
    "\n",
    "def infer_label(text, candidate_labels, fallback=\"unspecified\"):\n",
    "    if not text or len(text.strip()) < 20:\n",
    "        return fallback\n",
    "\n",
    "    try:\n",
    "        result = classifier(text[:2000], candidate_labels)\n",
    "        return result[\"labels\"][0]\n",
    "    except Exception:\n",
    "        return fallback\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 7. POPULATE PLURIVERSE JSON\n",
    "# =========================================================\n",
    "def populate_pluriverse_json(article_text):\n",
    "    output = copy.deepcopy(PLURIVERSE_SCHEMA)\n",
    "\n",
    "    # ---------- IDENTITY ----------\n",
    "    output[\"identity\"][\"name\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"citizens\", \"indigenous peoples\", \"workers\", \"scientists\", \"governments\", \"businesses\"]\n",
    "    )\n",
    "    output[\"identity\"][\"organization\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"government\", \"NGO\", \"corporation\", \"community group\", \"international body\"]\n",
    "    )\n",
    "\n",
    "    # ---------- INTERSECTION ----------\n",
    "    output[\"intersection\"][\"expression\"] = prompted_summary(\n",
    "        \"Describe how the situation is currently framed or expressed in the article.\",\n",
    "        article_text,\n",
    "        max_len=60\n",
    "    )\n",
    "    output[\"intersection\"][\"body_experience\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"health impact\", \"environmental exposure\", \"labor stress\", \"mental strain\"]\n",
    "    )\n",
    "    output[\"intersection\"][\"intention\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"adaptation\", \"resistance\", \"innovation\", \"reform\"]\n",
    "    )\n",
    "\n",
    "    # ---------- DEMOGRAPHIC ----------\n",
    "    output[\"demographic\"][\"race\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"indigenous\", \"minority\", \"global south\", \"global north\", \"global east\", \"global west\", \"unspecified\"]\n",
    "    )\n",
    "    output[\"demographic\"][\"gender\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"women\", \"men\", \"all genders\", \"unspecified\"]\n",
    "    )\n",
    "    output[\"demographic\"][\"education\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"formal education\", \"technical expertise\", \"traditional knowledge\"]\n",
    "    )\n",
    "    output[\"demographic\"][\"occupation\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"farmers\", \"workers\", \"scientists\", \"policy makers\", \"community leaders\"]\n",
    "    )\n",
    "    output[\"demographic\"][\"ability\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"disabled\", \"unspecified\", \"able\"]\n",
    "    )\n",
    "    output[\"demographic\"][\"wealth\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"low income\", \"middle income\", \"elite\", \"mixed\"]\n",
    "    )\n",
    "    output[\"demographic\"][\"health\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"vulnerable\", \"at risk\", \"stable\", \"protected\"]\n",
    "    )\n",
    "\n",
    "    # ---------- INTENTION ----------\n",
    "    output[\"intention\"][\"autonomy\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"self-determined\", \"externally imposed\"]\n",
    "    )\n",
    "    output[\"intention\"][\"design\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"top-down\", \"bottom-up\", \"co-created\"]\n",
    "    )\n",
    "    output[\"intention\"][\"future\"] = prompted_summary(\n",
    "        \"Summarize what future outcomes, scenarios, or trajectories are suggested.\",\n",
    "        article_text,\n",
    "        max_len=100\n",
    "    )\n",
    "    output[\"intention\"][\"new\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"technological innovation\", \"social innovation\", \"policy reform\"]\n",
    "    )\n",
    "    output[\"intention\"][\"symbol\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"progress\", \"resistance\", \"healing\", \"regeneration\"]\n",
    "    )\n",
    "    output[\"intention\"][\"impact\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"systemic change\", \"localized change\", \"symbolic change\"]\n",
    "    )\n",
    "    output[\"intention\"][\"support\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"public support\", \"institutional support\", \"grassroots support\"]\n",
    "    )\n",
    "\n",
    "    # ---------- POLICY CRITIQUE ----------\n",
    "    output[\"policy_critique\"][\"origin\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"colonial\", \"neoliberal\", \"state-led\", \"community-led\"]\n",
    "    )\n",
    "    output[\"policy_critique\"][\"leak\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"regulatory failure\", \"corruption\", \"resource extraction\"]\n",
    "    )\n",
    "    output[\"policy_critique\"][\"depletion\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"ecological depletion\", \"social depletion\", \"economic depletion\"]\n",
    "    )\n",
    "    output[\"policy_critique\"][\"obstacles\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"political resistance\", \"economic constraints\", \"cultural barriers\"]\n",
    "    )\n",
    "\n",
    "    # ---------- FUTURE ----------\n",
    "    output[\"future\"][\"ritual\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"restoration\", \"remembrance\", \"adaptation\", \"innovation\"]\n",
    "    )\n",
    "    output[\"future\"][\"hope\"] = prompted_summary(\n",
    "        \"Summarize what the article implies people hope for, value, or aspire to.\",\n",
    "        article_text,\n",
    "        max_len=60\n",
    "    )\n",
    "    output[\"future\"][\"depict\"] = infer_label(\n",
    "        article_text,\n",
    "        [\"utopian\", \"dystopian\", \"transitional\", \"resilient\"]\n",
    "    )\n",
    "\n",
    "    return output\n",
    "\n",
    "# =========================================================\n",
    "# 8. END-TO-END FUNCTION (DASHBOARD INPUT â†’ JSON OUTPUT)\n",
    "# =========================================================\n",
    "def analyze_article_to_pluriverse_json(article_url):\n",
    "    raw_text = scrape_article(article_url)\n",
    "    article_text = clean_article_text(raw_text)\n",
    "\n",
    "    if not article_text or len(article_text) < 100:\n",
    "        raise ValueError(\"Article text could not be reliably extracted.\")\n",
    "\n",
    "    populated_json = populate_pluriverse_json(article_text)\n",
    "    return populated_json\n",
    "\n",
    "\n",
    "# =========================================================\n",
    "# 9. RUN EXAMPLE\n",
    "# =========================================================\n",
    "ARTICLE_URL = \"https://spectator.org/how-did-summers-vetting-repeatedly-miss-epstein-ties/\"\n",
    "\n",
    "result = analyze_article_to_pluriverse_json(ARTICLE_URL)\n",
    "\n",
    "print(json.dumps(result, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "77f4ea48-6a7b-44f6-a6d6-c5e0a2066a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (4.57.3)\n",
      "Requirement already satisfied: torch in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: sentencepiece in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (0.2.1)\n",
      "Requirement already satisfied: filelock in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.13/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.13/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in ./.local/lib/python3.13/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.local/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Volumes/mac/opt/anaconda3/lib/python3.13/site-packages (from requests->transformers) (2025.10.5)\n",
      "Loaded 15 Pluriverse article entries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unified Pluriverse JSON saved to:\n",
      "Epstein_Pluriverse_avg_data.json\n",
      "{\n",
      "  \"identity\": {\n",
      "    \"url\": \"multiple sources\",\n",
      "    \"source\": \"left\",\n",
      "    \"name\": \"citizens\",\n",
      "    \"organization\": \"community group\"\n",
      "  },\n",
      "  \"intersection\": {\n",
      "    \"expression\": \"New York Times columnist David Brooks attended a dinner with Jeffrey Epstein in 2011. TikTok officials say they are looking into why many users have been unable to send the word \\\"Epstein\\\" in direct messages. California\",\n",
      "    \"body_experience\": \"health impact\",\n",
      "    \"intention\": \"resistance\"\n",
      "  },\n",
      "  \"demographic\": {\n",
      "    \"race\": \"minority\",\n",
      "    \"gender\": \"unspecified\",\n",
      "    \"education\": \"traditional knowledge\",\n",
      "    \"occupation\": \"policy makers\",\n",
      "    \"ability\": \"able\",\n",
      "    \"wealth\": \"mixed\",\n",
      "    \"health\": \"at risk\"\n",
      "  },\n",
      "  \"intention\": {\n",
      "    \"autonomy\": \"externally imposed\",\n",
      "    \"design\": \"co-created\",\n",
      "    \"future\": \"New York Times columnist David Brooks attended a dinner with Jeffrey Epstein in 2011. The day before the fatal shooting of Alex Pretti, TikTok users noticed they were unable to post videos that\",\n",
      "    \"new\": \"social innovation\",\n",
      "    \"symbol\": \"resistance\",\n",
      "    \"impact\": \"localized change\",\n",
      "    \"support\": \"public support\"\n",
      "  },\n",
      "  \"policy_critique\": {\n",
      "    \"origin\": \"community-led\",\n",
      "    \"leak\": \"resource extraction\",\n",
      "    \"depletion\": \"social depletion\",\n",
      "    \"obstacles\": \"political resistance\"\n",
      "  },\n",
      "  \"future\": {\n",
      "    \"ritual\": \"adaptation\",\n",
      "    \"hope\": \"TikTok is investigating why some users can't send the word \\\"Epstein\\\" in direct messages. California Gov. Gavin Newsom announced an inquiry into the\",\n",
      "    \"depict\": \"transitional\"\n",
      "  },\n",
      "  \"meta\": {\n",
      "    \"country\": \"\",\n",
      "    \"article_count\": 15,\n",
      "    \"method\": \"most-common categorical + synthesized narrative\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. INSTALL DEPENDENCIES (run once)\n",
    "# =========================================================\n",
    "!pip install transformers torch sentencepiece\n",
    "\n",
    "# =========================================================\n",
    "# 2. IMPORTS\n",
    "# =========================================================\n",
    "import json\n",
    "from statistics import mean\n",
    "from transformers import pipeline\n",
    "from collections import Counter\n",
    "\n",
    "# =========================================================\n",
    "# 3. FILE PATHS\n",
    "# =========================================================\n",
    "INPUT_PATH = \"Epstein_Pluriverse_news_data.json\"\n",
    "OUTPUT_PATH = \"Epstein_Pluriverse_avg_data.json\"\n",
    "\n",
    "# =========================================================\n",
    "# 4. LOAD DATA\n",
    "# =========================================================\n",
    "with open(INPUT_PATH, \"r\") as f:\n",
    "    articles = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(articles)} Pluriverse article entries\")\n",
    "\n",
    "# =========================================================\n",
    "# 5. LOAD SUMMARIZER\n",
    "# =========================================================\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"facebook/bart-large-cnn\"\n",
    ")\n",
    "\n",
    "# =========================================================\n",
    "# 6. HELPER FUNCTIONS\n",
    "# =========================================================\n",
    "def most_common(values, fallback=\"unspecified\"):\n",
    "    values = [v for v in values if isinstance(v, str) and v.strip()]\n",
    "    return Counter(values).most_common(1)[0][0] if values else fallback\n",
    "\n",
    "\n",
    "def synthesize_text(texts, max_len=60):\n",
    "    texts = [t for t in texts if isinstance(t, str) and len(t.strip()) > 30]\n",
    "    if not texts:\n",
    "        return \"insufficient signal\"\n",
    "\n",
    "    combined = \" \".join(texts)[:3500]\n",
    "\n",
    "    try:\n",
    "        summary = summarizer(\n",
    "            combined,\n",
    "            max_length=max_len,\n",
    "            min_length=25,\n",
    "            do_sample=False\n",
    "        )\n",
    "        return summary[0][\"summary_text\"]\n",
    "    except Exception:\n",
    "        return texts[0]\n",
    "\n",
    "# =========================================================\n",
    "# 7. AGGREGATE PLURIVERSE ENTRIES (MATCHING YOUR SCHEMA)\n",
    "# =========================================================\n",
    "def aggregate_pluriverse(entries):\n",
    "    unified = {\n",
    "        \"identity\": {},\n",
    "        \"intersection\": {},\n",
    "        \"demographic\": {},\n",
    "        \"intention\": {},\n",
    "        \"policy_critique\": {},\n",
    "        \"future\": {},\n",
    "        \"meta\": {\n",
    "            \"country\": \"\",\n",
    "            \"article_count\": len(entries),\n",
    "            \"method\": \"most-common categorical + synthesized narrative\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # ---------------- IDENTITY ----------------\n",
    "    unified[\"identity\"][\"url\"] = \"multiple sources\"\n",
    "    unified[\"identity\"][\"source\"] = most_common(\n",
    "        [e.get(\"identity\", {}).get(\"source\") for e in entries]\n",
    "    )\n",
    "    unified[\"identity\"][\"name\"] = most_common(\n",
    "        [e.get(\"identity\", {}).get(\"name\") for e in entries]\n",
    "    )\n",
    "    unified[\"identity\"][\"organization\"] = most_common(\n",
    "        [e.get(\"identity\", {}).get(\"organization\") for e in entries]\n",
    "    )\n",
    "\n",
    "    # ---------------- INTERSECTION ----------------\n",
    "    unified[\"intersection\"][\"expression\"] = synthesize_text(\n",
    "        [e.get(\"intersection\", {}).get(\"expression\") for e in entries],\n",
    "        max_len=45\n",
    "    )\n",
    "    unified[\"intersection\"][\"body_experience\"] = most_common(\n",
    "        [e.get(\"intersection\", {}).get(\"body_experience\") for e in entries]\n",
    "    )\n",
    "    unified[\"intersection\"][\"intention\"] = most_common(\n",
    "        [e.get(\"intersection\", {}).get(\"intention\") for e in entries]\n",
    "    )\n",
    "\n",
    "    # ---------------- DEMOGRAPHIC ----------------\n",
    "    for field in [\n",
    "        \"race\", \"gender\", \"education\", \"occupation\",\n",
    "        \"ability\", \"wealth\", \"health\"\n",
    "    ]:\n",
    "        unified[\"demographic\"][field] = most_common(\n",
    "            [e.get(\"demographic\", {}).get(field) for e in entries]\n",
    "        )\n",
    "\n",
    "    # ---------------- INTENTION ----------------\n",
    "    unified[\"intention\"][\"autonomy\"] = most_common(\n",
    "        [e.get(\"intention\", {}).get(\"autonomy\") for e in entries]\n",
    "    )\n",
    "    unified[\"intention\"][\"design\"] = most_common(\n",
    "        [e.get(\"intention\", {}).get(\"design\") for e in entries]\n",
    "    )\n",
    "    unified[\"intention\"][\"future\"] = synthesize_text(\n",
    "        [e.get(\"intention\", {}).get(\"future\") for e in entries],\n",
    "        max_len=40\n",
    "    )\n",
    "    unified[\"intention\"][\"new\"] = most_common(\n",
    "        [e.get(\"intention\", {}).get(\"new\") for e in entries]\n",
    "    )\n",
    "    unified[\"intention\"][\"symbol\"] = most_common(\n",
    "        [e.get(\"intention\", {}).get(\"symbol\") for e in entries]\n",
    "    )\n",
    "    unified[\"intention\"][\"impact\"] = most_common(\n",
    "        [e.get(\"intention\", {}).get(\"impact\") for e in entries]\n",
    "    )\n",
    "    unified[\"intention\"][\"support\"] = most_common(\n",
    "        [e.get(\"intention\", {}).get(\"support\") for e in entries]\n",
    "    )\n",
    "\n",
    "    # ---------------- POLICY CRITIQUE ----------------\n",
    "    for field in [\"origin\", \"leak\", \"depletion\", \"obstacles\"]:\n",
    "        unified[\"policy_critique\"][field] = most_common(\n",
    "            [e.get(\"policy_critique\", {}).get(field) for e in entries]\n",
    "        )\n",
    "\n",
    "    # ---------------- FUTURE ----------------\n",
    "    unified[\"future\"][\"ritual\"] = most_common(\n",
    "        [e.get(\"future\", {}).get(\"ritual\") for e in entries]\n",
    "    )\n",
    "    unified[\"future\"][\"hope\"] = synthesize_text(\n",
    "        [e.get(\"future\", {}).get(\"hope\") for e in entries],\n",
    "        max_len=35\n",
    "    )\n",
    "    unified[\"future\"][\"depict\"] = most_common(\n",
    "        [e.get(\"future\", {}).get(\"depict\") for e in entries]\n",
    "    )\n",
    "\n",
    "    return unified\n",
    "\n",
    "# =========================================================\n",
    "# 8. RUN AGGREGATION\n",
    "# =========================================================\n",
    "unified_pluriverse = aggregate_pluriverse(articles)\n",
    "\n",
    "# =========================================================\n",
    "# 9. SAVE OUTPUT\n",
    "# =========================================================\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    json.dump(unified_pluriverse, f, indent=2)\n",
    "\n",
    "print(\"Unified Pluriverse JSON saved to:\")\n",
    "print(OUTPUT_PATH)\n",
    "\n",
    "# =========================================================\n",
    "# 10. DISPLAY RESULT\n",
    "# =========================================================\n",
    "print(json.dumps(unified_pluriverse, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e870f06-1342-4ba4-b47b-58ce61075c32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
